---
title: SNAPSHOT ENSEMBLES：TRAIN 1, GET M FOR FREE
tags: 2020-7-24
renderNumberedHeading: true
grammar_cjkRuby: true
---


**知识点补充**

   *梯度更新方式：*
   
 1. Batch gradient descent 批梯度下降:
     由于在训练时数据集太大了，无法将这些数据一次性放到网络中进行训练，所以此时需要将数据分成几个batch，分别送进网络中进行训练，此时通过计算每个batch数据的损失梯度值来更新各个参数，效率低，容易造成内存负担过重；
	 
 2. 随机梯度下降法 SGD
     每训练一个数据就计算一下损失值来求梯度更新参数，该方法速度比较块，收敛性能不太好，始终在最优点附近晃来晃去，达不到最优点，两次参数的更新也可能互相抵消掉，造成目标函数震荡的比较剧烈；
 3. mini batch gradient descent 
     为了折中上面两种梯度下降法的优缺点，使用mini batch 梯度下降法，这种方法把数据分为若干小的批次，按批来更新参数，一个批中的一组数据共同决定了本次梯度的方向，下降起来就不容易跑偏，减少了随机性；另一方面，因为批的样本数与整个数据集相比小了很多，计算量也不是很大。